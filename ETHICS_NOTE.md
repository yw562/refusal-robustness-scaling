# Ethics Note

This repository documents research in **AI Safety**, specifically on the **scaling behavior of refusal robustness** in large language models (LLMs).

## Research Purpose

- The primary aim is to understand how reliably models **refuse harmful requests** under different conditions (e.g., scale, attack compute).  
- This research is intended to **improve AI robustness** and inform the design of safer models, not to enable harmful capabilities.

## Safety Practices

- **No sensitive content**: All prompts and responses included in this repository are either **synthetic** or from **public benchmarks** that are widely used in academic safety research.  
- **No misuse**: The repository does not provide, encourage, or endorse harmful instructions.  
- **Transparency**: All datasets and metrics are documented, with clear statements about their safe and ethical use.

## Responsible Use

We kindly ask any users of this repository to:
1. Use the code and datasets strictly for **research on AI robustness and safety**.  
2. Avoid generating or sharing real-world harmful content.  
3. Follow the ethical guidelines of their institution and the broader AI Safety community.

---

By following these practices, we aim to ensure that this research contributes positively to the development of **trustworthy and aligned AI systems**.
