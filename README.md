# ğŸš€ Refusal Robustness Scaling

## Quickstart

### 1. Install environment
```bash
make setup

2. Prepare data
Place your evaluation set at:

bash
Copy code
data/raw/redteam_eval.csv
3. Run experiments
Example: Qwen2-7B LoRA grid (Day7)

bash
Copy code
./experiments/qwen2_7b_lora_grid/run.sh
4. Inspect results
Metrics and outputs are saved under:

bash
Copy code
experiments/qwen2_7b_lora_grid/results/
ğŸ“Š Key Metrics
We use three governance-relevant refusal robustness metrics:

RRR (Refusal Robustness Rate): fraction of harmful prompts successfully refused.

RD (Representation Distance): median embedding drift (SBERT cosine).

CE (Compliance Error): 1 â€“ refusal rate (compliance with harmful requests).

ğŸ”¬ Experimental Highlights
Baseline (Day7, Qwen2-7B)
RRR â‰ˆ 0.13 â†’ partial but non-trivial robustness

CE â‰ˆ 0.87

RD (median) = moderate drift â†’ pretrained model has some refusal robustness

After Adversarial LoRA Fine-Tuning
(s500, s1000, s2000 steps)

RRR â†’ 0.0 (collapse)

CE â†’ 1.0 (full compliance with harmful requests)

RD â†’ NaN (refusal subspace destroyed, safety not recovered)

â¡ï¸ Training longer reduces loss but does not restore safety.
â¡ï¸ Refusal robustness collapses under LoRA attack regardless of steps.

Cross-Model Scaling (Day3â€“Day6)
TinyLlama-1.1B, Phi-3-mini-3.8B show the same collapse patterns.

Attacker compute (LoRA steps/params) dominates model size in determining robustness.

ğŸ“‰ Key Figures (see results/figures/)
RRR_compare.png: refusal robustness collapse curves

CE_compare.png: compliance error vs steps

RD_compare.png: representation drift saturation

Scaling plots: refusal robustness across 1Bâ€“7B models

ğŸ” Reproducibility
Each dayâ€™s experiment tracked with notebooks + scripts + metrics (JSON/CSV).

make_figs.py and make_figs_from_combined.py regenerate all plots from results.

results/metrics/ contains standardized CSVs (results_grid.csv, results_grid_qwen2_7b.csv, etc.).

ğŸ› Governance Relevance
Refusal robustness can be completely defeated by small adversarial compute (LoRA adapters).

Model scaling (1B â†’ 7B) does not guarantee stronger safety.

Highlights the need for tampering evaluations and adversarial stress-tests in AI governance frameworks.

