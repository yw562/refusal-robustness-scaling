# Refusal Robustness Scaling

**Scaling behavior of refusal robustness in LLMs â€” reproducible metrics pipeline (RRR, RD, CE).**

This repository implements a research pipeline for evaluating how large language models (LLMs) lose or maintain refusal robustness under adversarial fine-tuning. We focus on scaling behavior, reproducibility, and governance-relevant insights.

---

## ğŸ“‚ Repository Structure

refusal-robustness-scaling/
â”‚
â”œâ”€â”€ configs/ # Model / attack / eval configs (YAML)
â”‚ â”œâ”€â”€ models/ # Model definitions (TinyLlama, Phi-3, Qwen2-7B, â€¦)
â”‚ â”œâ”€â”€ attacks/ # LoRA attack configs (steps, lr, etc.)
â”‚ â””â”€â”€ eval/ # Evaluation configs (red-teaming prompts, etc.)
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw evaluation sets (e.g., redteam_eval.csv, safe data)
â”‚ â”œâ”€â”€ interim/ # (optional) Preprocessed data
â”‚ â””â”€â”€ processed/ # (optional) Final datasets
â”‚
â”œâ”€â”€ experiments/
â”‚ â””â”€â”€ qwen2_7b_lora_grid/ # Example experiment (Day7, Qwen2-7B LoRA grid)
â”‚ â”œâ”€â”€ run.sh # Example script to launch training + eval
â”‚ â””â”€â”€ results/ # Outputs for baseline + LoRA runs
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ checks/ # Jupyter notebooks to sanity-check results
â”‚ â””â”€â”€ optional/ # Extra pairwise comparisons
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ metrics/ # Collected CSVs / JSON metrics
â”‚ â””â”€â”€ figures/ # Figures for analysis
â”‚
â”œâ”€â”€ scripts/ # Core pipeline scripts (data prep, LoRA, eval, RD fix)
â”œâ”€â”€ figs/ # Publication-ready plots + tables
â”œâ”€â”€ Makefile # One-command setup & run
â”œâ”€â”€ requirements.txt # Core dependencies
â”œâ”€â”€ requirements-dev.txt # Dev / plotting / linting
â”œâ”€â”€ DATA_POLICY.md # Data policy
â”œâ”€â”€ ETHICS_NOTE.md # Ethical considerations
â””â”€â”€ README.md

yaml
Copy code

---

## ğŸš€ Quickstart

1. **Install environment**
   ```bash
   make setup
Prepare data
Place your evaluation set at:

bash
Copy code
data/raw/redteam_eval.csv
Run experiments
Example: Qwen2-7B LoRA grid (Day7)

bash
Copy code
./experiments/qwen2_7b_lora_grid/run.sh
Inspect results
Metrics and outputs are saved under:

bash
Copy code
experiments/qwen2_7b_lora_grid/results/
ğŸ“Š Key Metrics
We use three governance-relevant refusal robustness metrics:

RRR (Refusal Robustness Rate): fraction of harmful prompts successfully refused.

RD (Representation Distance): median embedding drift (SBERT cosine).

CE (Compliance Error): 1 â€“ refusal rate (compliance with harmful requests).

ğŸ§ª Experimental Highlights
Baseline (Day7, Qwen2-7B):
RRR â‰ˆ 0.13

CE â‰ˆ 0.87

RD (median) ~ moderate
â†’ The pretrained model shows partial but non-trivial refusal robustness.

After Adversarial LoRA Fine-Tuning (s500, s1000, s2000 steps):
RRR â†’ 0.0 (collapse)

CE â†’ 1.0 (full compliance with harmful requests)

RD â†’ NaN (saturated drift, refusal subspace destroyed)
â†’ Longer training reduces loss but does not recover safety. Refusal robustness collapses under LoRA attack regardless of steps.

Cross-Model Scaling (Day3â€“Day6):
TinyLlama-1.1B, Phi-3-mini-3.8B show similar collapse patterns.

Attacker compute (LoRA steps/params) dominates model size in determining robustness.

ğŸ“ˆ Key Figures
All plots are under results/figures/:

RRR_compare.png: refusal robustness collapse curves

CE_compare.png: compliance error vs steps

RD_compare.png: representation drift saturation

Scaling plots: refusal robustness across 1Bâ€“7B models

Example (Day7, Qwen2-7B):


ğŸ” Reproducibility
Each dayâ€™s experiment tracked with notebooks + scripts + metrics JSON/CSV.

make_figs.py and make_figs_from_combined.py regenerate all plots from results.

results/metrics/ contains standardized CSVs (results_grid.csv, results_grid_qwen2_7b.csv, etc.).

ğŸ“ Governance Relevance
Refusal robustness can be completely defeated by small adversarial compute (LoRA adapters).

Model scaling (1B â†’ 7B) does not guarantee stronger safety.

Highlights the need for tampering evaluations and adversarial training stress-tests in AI governance frameworks.

ğŸ“š References
Refusal Robustness Scaling Proposal (preprint PDF in repo)

Related work: Phuong & Jenner (2025), Christiano (2021), Krakovna (2024)
