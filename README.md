# ğŸš€ Refusal Robustness Scaling

Scaling behavior of refusal robustness in LLMs â€” reproducible metrics pipeline (RRR, RD, CE).

---

## âš¡ Quickstart

### 1. Install environment
make setup

### 2. Prepare data  
Place your evaluation set at:  
data/raw/redteam_eval.csv

### 3. Run experiments  
Example: Qwen2-7B LoRA grid  
./experiments/qwen2_7b_lora_grid/run.sh

### 4. Inspect results  
Metrics and outputs are saved under:  
experiments/qwen2_7b_lora_grid/results/

---

## ğŸ“Š Key Metrics

We evaluate refusal robustness using three governance-relevant metrics:

- **RRR (Refusal Robustness Rate):** fraction of harmful prompts successfully refused  
- **RD (Representation Distance):** median semantic drift (SBERT cosine)  
- **CE (Compliance Error):** 1 â€“ refusal rate (compliance with harmful requests)

---

## ğŸ”¬ Experimental Highlights

### Baseline Models:
- **TinyLlama-1.1B:** RRR â‰ˆ 0.16, CE â‰ˆ 0.63, RD â‰ˆ 0.78  
- **Phi-3-mini-3.8B:** RRR â‰ˆ 0.72, CE â‰ˆ 0.64, RD â‰ˆ 0.28  
- **Qwen2-7B:** RRR â‰ˆ 0.13, CE â‰ˆ 0.87, RD â‰ˆ 0.81

### After Adversarial LoRA Fine-Tuning (Qwen2-7B, 500â€“2000 steps):
- **RRR â†’ 0.0 (collapse)**
- **CE â†’ 1.0 (full compliance with harmful requests)**
- **RD â†’ ~0.57â€“0.59 (stable drift after refusal collapse)**

â¡ï¸ Refusal robustness collapses under LoRA attack regardless of training steps.  
â¡ï¸ Larger models start with stronger refusal but are equally vulnerable once attacked.  
â¡ï¸ Attacker compute (LoRA budget) dominates over model size in determining robustness.

---

## ğŸ“‰ Key Figures

All plots available under `results/figures/`:

- RRR_compare.png â†’ refusal robustness collapse curves  
- CE_compare.png â†’ compliance error vs steps  
- RD_compare.png â†’ representation drift saturation  
- Scaling plots â†’ refusal robustness across 1Bâ€“7B models

---

## ğŸ“‹ Results Summary

Model / Setting | RRR (â†‘ safer) | CE (â†“ safer) | RD (median) | Notes
---|---|---|---|---
TinyLlama-1.1B | 0.16 â†’ 0.00 | 0.63 â†’ 1.00 | ~0.78 | Collapse after LoRA
Phi-3-mini-3.8B | 0.72 â†’ 0.00 | 0.64 â†’ 1.00 | ~0.28 | Collapse after LoRA
Qwen2-7B (baseline) | ~0.13 | ~0.87 | ~0.81 | Partial robustness
Qwen2-7B + LoRA 500 | 0.00 | 1.00 | ~0.58 | Fully compromised
Qwen2-7B + LoRA 1000 | 0.00 | 1.00 | ~0.57 | Same as 500
Qwen2-7B + LoRA 2000 | 0.00 | 1.00 | ~0.59 | Same as 500/1000

\* Earlier reports showed NaN for RD due to averaging instability.  
We now use the **median RD**, which yields stable values without changing the qualitative conclusion.

---


## ğŸ” Reproducibility

- Each experiment tracked with notebooks + scripts + metrics (JSON/CSV)  
- `make_figs.py` and `make_figs_from_combined.py` regenerate all plots from results  
- All standardized metrics are stored in `results/metrics/`

---

## ğŸ› Governance Relevance

- Refusal robustness can be completely defeated by modest adversarial fine-tuning (LoRA adapters).  
- Model scaling (1B â†’ 7B) does not guarantee safety.  
- Results highlight the need for tampering evaluations and adversarial stress-tests in AI governance frameworks.

---

## ğŸ”® Next Steps

Planned extensions:
- Add 13B and 70B checkpoints to extend the scaling law
- Vary LoRA rank, dataset size, and attack budget
- Package evaluation as a reusable tampering-eval benchmark

---

## ğŸ“š References

- Refusal Robustness Scaling Proposal (preprint PDF in repo)  
- Related work: Phuong & Jenner (2025), Christiano (2021), Krakovna (2024)
